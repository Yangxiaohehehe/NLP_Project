# flash attn文件过大 无法git
pip install flash_attn-2.5.9.post1+cu122torch2.1cxx11abiFALSE-cp310-cp310-linux_x86_64.whl
需要去
https://github.com/Dao-AILab/flash-attention/releases?page=3
下载对应版本